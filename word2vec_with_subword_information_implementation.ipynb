{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec with subword information implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zyzhang1992/NLP/blob/master/word2vec_with_subword_information_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mo0FnfShjoZ",
        "colab_type": "text"
      },
      "source": [
        "# WORD2VEC SKIPGRAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuj6Hq05g2Xy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "IMPORTS\n",
        "'''\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.functional as F\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyB_B5qphprg",
        "colab_type": "code",
        "outputId": "2dd429d4-5e48-472c-c6cf-45fbc2a7d39d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esDum-XDhwgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Defining Corpus\n",
        "'''\n",
        "\n",
        "corpus = [\n",
        "    'he is a king',\n",
        "    'she is a queen',\n",
        "    'he is a man',\n",
        "    'she is a woman',\n",
        "    'warsaw is poland capital',\n",
        "    'berlin is germany capital',\n",
        "    'paris is france capital',   \n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b6PAEIahzCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Tokenizing corpus\n",
        "'''\n",
        "\n",
        "def tokenize_corpus(corpus):\n",
        "    tokens = [x.split() for x in corpus]\n",
        "    return tokens\n",
        "\n",
        "tokenized_corpus = tokenize_corpus(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SS1sV6ah1TL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Generating Vocabulary\n",
        "'''\n",
        "\n",
        "vocabulary = []\n",
        "for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "\n",
        "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
        "\n",
        "vocabulary_size = len(vocabulary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2LvroFplbyB",
        "colab_type": "code",
        "outputId": "eb72a10d-c3bb-457e-e0e7-20bffa11be08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocabulary_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eXnpk7th3hc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 2\n",
        "idx_pairs = []\n",
        "# for each sentence\n",
        "for sentence in tokenized_corpus:\n",
        "    indices = [word2idx[word] for word in sentence]\n",
        "    # for each word, threated as center word\n",
        "    for center_word_pos in range(len(indices)):\n",
        "        # for each window position\n",
        "        for w in range(-window_size, window_size + 1):\n",
        "            context_word_pos = center_word_pos + w\n",
        "            # make soure not jump out sentence\n",
        "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                continue\n",
        "            context_word_idx = indices[context_word_pos]\n",
        "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
        "\n",
        "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzqa1OYDh7-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ONE HOT ENCODING FOR INPUT\n",
        "\n",
        "def get_input_layer(word_idx):\n",
        "    x = torch.zeros(vocabulary_size).float()\n",
        "    x[word_idx] = 1.0\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80CGg69LiVTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dims = 8\n",
        "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
        "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
        "num_epochs = 1000\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeWu133nicY7",
        "colab_type": "code",
        "outputId": "3a4530a1-3b33-4306-c954-eadb90b273e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "for epo in range(num_epochs):\n",
        "    loss_val = 0\n",
        "    for data, target in idx_pairs:\n",
        "        \n",
        "        x = Variable(get_input_layer(data)).float()\n",
        "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
        "       \n",
        "        z1 = torch.matmul(W1, x)\n",
        "        z2 = torch.matmul(W2, z1)\n",
        "    \n",
        "        log_softmax = F.log_softmax(z2, dim=0)\n",
        "        \n",
        "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
        "        \n",
        "        loss_val += loss.item()\n",
        "        loss.backward()\n",
        "        W1.data -= learning_rate * W1.grad.data\n",
        "        W2.data -= learning_rate * W2.grad.data\n",
        "\n",
        "        W1.grad.data.zero_()\n",
        "        W2.grad.data.zero_()\n",
        "    if epo % 100 == 0:    \n",
        "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at epo 0: 5.605964619772775\n",
            "Loss at epo 100: 2.757629302569798\n",
            "Loss at epo 200: 2.285395928791591\n",
            "Loss at epo 300: 2.046659631388528\n",
            "Loss at epo 400: 1.9045629620552063\n",
            "Loss at epo 500: 1.8133672765323094\n",
            "Loss at epo 600: 1.7528529235294887\n",
            "Loss at epo 700: 1.7106595754623413\n",
            "Loss at epo 800: 1.6800211565835135\n",
            "Loss at epo 900: 1.657117143699101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Seb2CbpCmLvj",
        "colab_type": "code",
        "outputId": "0c3ebcf3-3496-4950-de32-d69e3cb85aa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "idx2word"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'he',\n",
              " 1: 'is',\n",
              " 2: 'a',\n",
              " 3: 'king',\n",
              " 4: 'she',\n",
              " 5: 'queen',\n",
              " 6: 'man',\n",
              " 7: 'woman',\n",
              " 8: 'warsaw',\n",
              " 9: 'poland',\n",
              " 10: 'capital',\n",
              " 11: 'berlin',\n",
              " 12: 'germany',\n",
              " 13: 'paris',\n",
              " 14: 'france'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrThZlPNl39C",
        "colab_type": "code",
        "outputId": "e64ab2e7-2d1f-4988-e1d1-66b40e90e43f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "W2.data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4123, -0.0245,  1.8233,  1.3324,  1.2817, -0.3725,  1.6364, -0.9389],\n",
              "        [-1.9488, -0.9129, -2.2870, -0.3702,  0.8873,  0.0932,  1.1944, -0.4622],\n",
              "        [-0.3428, -1.5614, -0.4646, -0.9458, -0.3248, -0.5454, -0.1858, -2.3451],\n",
              "        [ 0.5922,  0.0228,  0.9677,  0.6391,  0.0165, -0.2109,  1.2660,  1.0933],\n",
              "        [-0.1302,  0.8932,  0.1873,  0.7610, -1.3264, -0.6507, -0.5616,  1.0430],\n",
              "        [ 1.7140,  1.5295,  1.6724,  0.3624, -0.6883, -1.8532,  0.9596, -0.5718],\n",
              "        [ 0.8975,  0.9169,  0.1767, -0.3585, -1.9290,  1.1728,  0.3248, -0.6751],\n",
              "        [-0.3544,  0.5478, -0.5784, -0.0673, -0.9605, -0.3760, -0.3123, -0.0997],\n",
              "        [-0.4670, -0.6457,  0.1371, -0.5279, -0.5776, -0.0161, -0.7660,  0.7703],\n",
              "        [ 1.7737, -0.5718, -0.3229, -1.9442, -1.4522, -0.4140, -0.3360,  0.0713],\n",
              "        [-0.2404, -0.7714, -0.3713,  0.2627,  0.1469,  0.1600, -0.5915,  1.5953],\n",
              "        [-0.0424,  2.2461,  0.0638,  1.8172,  1.6007,  1.4632, -0.3946,  1.4902],\n",
              "        [ 0.6722,  1.4548, -0.9361,  0.1554,  0.7772,  0.8190,  0.5075, -0.3873],\n",
              "        [-0.0903,  0.8014,  0.6531, -0.3741,  0.1951, -0.0875, -0.4937, -1.0758],\n",
              "        [ 0.5790, -0.8224, -0.2038, -0.9287,  0.6794,  0.9748, -0.8341,  0.2046]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaNFMm5jl6Eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_analogy(a,b,c,d):\n",
        "  ind_a, ind_b, ind_c, ind_d = word2idx[a], word2idx[b], word2idx[c], word2idx[d]\n",
        "  \n",
        "  score_vector = (W2.data[ind_a] - W2.data[ind_b]) /  (W2.data[ind_c] - W2.data[ind_d])\n",
        "  \n",
        "  return score_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9rUvR08nMr9",
        "colab_type": "code",
        "outputId": "4c7605be-39c7-4752-9aeb-0c19cf6721a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score = calculate_analogy('he', 'she', 'king', 'queen')\n",
        "score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.2515,  0.6091, -2.3216,  2.0647,  3.7002,  0.1694,  7.1732, -1.1903])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N38K17dcnoUi",
        "colab_type": "text"
      },
      "source": [
        "# WORD2VEC WITH SUBWORD INFORMATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2BRv59GnWY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 2\n",
        "n_gram_vocab = []\n",
        "\n",
        "for word in vocabulary:\n",
        "  word = '<' + word + '>'\n",
        "  \n",
        "  n_gram_bag = []\n",
        "  \n",
        "  for index in range(len(word)):\n",
        "    if (index+n) <= len(word):\n",
        "      char_string = ''\n",
        "      for count in range(n):         \n",
        "        char_string += (word[index+count])\n",
        "      n_gram_bag.append(char_string)\n",
        "  n_gram_bag.append(word)    \n",
        "  n_gram_vocab.append(n_gram_bag)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-oP25ENqDri",
        "colab_type": "code",
        "outputId": "73d352c7-64c4-4330-93e0-a431c2c35918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "n_gram_vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<h', 'he', 'e>', '<he>'],\n",
              " ['<i', 'is', 's>', '<is>'],\n",
              " ['<a', 'a>', '<a>'],\n",
              " ['<k', 'ki', 'in', 'ng', 'g>', '<king>'],\n",
              " ['<s', 'sh', 'he', 'e>', '<she>'],\n",
              " ['<q', 'qu', 'ue', 'ee', 'en', 'n>', '<queen>'],\n",
              " ['<m', 'ma', 'an', 'n>', '<man>'],\n",
              " ['<w', 'wo', 'om', 'ma', 'an', 'n>', '<woman>'],\n",
              " ['<w', 'wa', 'ar', 'rs', 'sa', 'aw', 'w>', '<warsaw>'],\n",
              " ['<p', 'po', 'ol', 'la', 'an', 'nd', 'd>', '<poland>'],\n",
              " ['<c', 'ca', 'ap', 'pi', 'it', 'ta', 'al', 'l>', '<capital>'],\n",
              " ['<b', 'be', 'er', 'rl', 'li', 'in', 'n>', '<berlin>'],\n",
              " ['<g', 'ge', 'er', 'rm', 'ma', 'an', 'ny', 'y>', '<germany>'],\n",
              " ['<p', 'pa', 'ar', 'ri', 'is', 's>', '<paris>'],\n",
              " ['<f', 'fr', 'ra', 'an', 'nc', 'ce', 'e>', '<france>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeEFYCnIqlMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_seq_vocab = []\n",
        "\n",
        "for word in n_gram_vocab:\n",
        "  \n",
        "  for char_seq in word:\n",
        "    if char_seq not in char_seq_vocab:\n",
        "      char_seq_vocab.append(char_seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsKDTUCas9UP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_seq_vocab_size = len(char_seq_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnTFSxAzuPQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_seq2idx = {w: idx for (idx, w) in enumerate(char_seq_vocab)}\n",
        "idx2char_seq = {idx: w for (idx, w) in enumerate(char_seq_vocab)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqX2uq5ts-YA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ONE HOT ENCODING FOR INPUT\n",
        "\n",
        "def get_input_layer_char_seq(char_seq_idx):\n",
        "    x = torch.zeros(char_seq_vocab_size).float()\n",
        "    x[char_seq_idx] = 1.0\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2DLioaZus8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dims = 8\n",
        "W1 = Variable(torch.randn(embedding_dims, char_seq_vocab_size).float(), requires_grad=True)\n",
        "W2 = Variable(torch.randn(char_seq_vocab_size, embedding_dims).float(), requires_grad=True)\n",
        "num_epochs = 1000\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBgoCIvrvQIh",
        "colab_type": "code",
        "outputId": "65a67a70-ffb6-45a9-864d-e3a4abd88493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "for epo in range(num_epochs):\n",
        "    loss_val = 0\n",
        "    for data, target in idx_pairs:\n",
        "        Z1 = 0.0\n",
        "        for i in range(len(n_gram_vocab[data])):\n",
        "          char_seq_vec = Variable(get_input_layer_char_seq(i)).float()\n",
        "          Z1 += torch.matmul(W1, char_seq_vec)\n",
        "        #x = Variable(get_input_layer(data)).float()\n",
        "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
        "       \n",
        "        #z1 = torch.matmul(W1, x)\n",
        "        Z2 = torch.matmul(W2, Z1)\n",
        "    \n",
        "        log_softmax = F.log_softmax(Z2, dim=0)\n",
        "        \n",
        "        loss_2 = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
        "        \n",
        "        loss_val += loss_2.item()\n",
        "        loss_2.backward(retain_graph=True)\n",
        "        W1.data -= learning_rate * W1.grad.data\n",
        "        W2.data -= learning_rate * W2.grad.data\n",
        "\n",
        "        W1.grad.data.zero_()\n",
        "        W2.grad.data.zero_()\n",
        "    if epo % 100 == 0:    \n",
        "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at epo 0: 12.607089247022357\n",
            "Loss at epo 100: 2.8115211997713363\n",
            "Loss at epo 200: 2.3070071220397947\n",
            "Loss at epo 300: 2.079699775150844\n",
            "Loss at epo 400: 1.9827991894313268\n",
            "Loss at epo 500: 1.9414680276598248\n",
            "Loss at epo 600: 1.9159873689923967\n",
            "Loss at epo 700: 1.8986946037837438\n",
            "Loss at epo 800: 1.8863659926823206\n",
            "Loss at epo 900: 1.8772497790200369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jXXzzIfvqNF",
        "colab_type": "code",
        "outputId": "cfbf7241-a8b8-41d2-8fc4-f8674bf710ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "W2.data[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4758,  0.0456,  0.4351,  1.5379,  0.0725, -0.0205,  1.5503, -0.7806],\n",
              "        [ 2.2069, -0.4227,  0.4275, -0.2044, -0.2550,  1.9400,  0.3378, -1.2663],\n",
              "        [ 2.4431,  0.0764,  2.4822, -1.0189, -1.1074, -1.7477,  0.1908, -0.4770],\n",
              "        [ 0.8356,  1.3642,  1.3098,  1.6873,  0.2735, -0.8131,  0.3572,  0.1301],\n",
              "        [ 2.1187, -0.1521,  1.0664,  1.4335,  3.2855, -1.0207,  0.1699,  0.1405]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iHgc9JTzptO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_word_vector(word):\n",
        "  index = word2idx[word]\n",
        "  for i in range(len(n_gram_vocab[index])):\n",
        "    word_vector = 0.0\n",
        "    char_seq_ind = char_seq2idx[n_gram_vocab[index][i]]\n",
        "    word_vector += W2.data[char_seq_ind]\n",
        "    \n",
        "    return word_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa9p_Rnb0k7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_analogy_2(a,b,c,d):\n",
        "  \n",
        "  score_vector = (calculate_word_vector(a) - calculate_word_vector(b)) /  (calculate_word_vector(c) - calculate_word_vector(d))\n",
        "  return score_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iMSV4SA0uD4",
        "colab_type": "code",
        "outputId": "796235dd-8242-439c-c045-edac01a6c0bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score = calculate_analogy_2('he', 'she', 'king', 'queen')\n",
        "score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.0503, -2.8577, -0.0325,  2.3783, -0.1267,  0.8438,  7.8896,  1.1950])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IywjFA4l1L5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}